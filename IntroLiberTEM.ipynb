{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12876eb0",
   "metadata": {},
   "source": [
    "# Introduction to LiberTEM\n",
    "\n",
    "Inspired by https://github.com/LiberTEM/LiberTEM/blob/master/examples/Introduction%20to%20UDFs.ipynb and https://libertem.github.io/LiberTEM-blobfinder/strainmap-SiGe.html.\n",
    "\n",
    "This notebook should be self-contained, but requires an internet connection to download a public Merlin Medipix dataset. The dataset is extracted into the current directory under the folder `'./dataset'`.\n",
    "\n",
    "## What is LiberTEM ???\n",
    "\n",
    "LiberTEM is an open-source library for high-speed data processing which is specialised towards 4D-STEM datasets, but in fact is capable of efficiently carrying out any task where the computation can be performed independently on different pieces of the dataset before final merging into a result.\n",
    "\n",
    "The main project page can be found here https://libertem.github.io/LiberTEM/.\n",
    "\n",
    "The project is primarily run by the FZ JÃ¼lich research centre in Germany, but is open-source so is freely available and anyone can contribute or request features.\n",
    "\n",
    "## Documentation\n",
    "\n",
    "The documentation is mainly found on the above site, though for completeness also check out the GitHub page where the code is stored and all discussion about features / bugs etc happens: https://github.com/LiberTEM/LiberTEM/.\n",
    "\n",
    "## Concepts\n",
    "\n",
    "#### Partitions\n",
    "\n",
    "LiberTEM is based on a `map--reduce` programming model, which means the data to process is broken up into chunks (partitions) which can be independently processed (*map*) into intermediate results, which are then combined into a single result (usually) much smaller than the datset itself (*reduce*).\n",
    "\n",
    "#### Clusters\n",
    "\n",
    "LiberTEM is based on the Dask framework for parallel computing (https://dask.org/), which makes it very flexible and easy to scale from a single machine to multi-machine / cluster settings. In addition, LiberTEM supports running functions using CuPy (https://cupy.dev/) to access GPU-computing resources without changes to the code, allowing execution on hybrid CPU / GPU clusters with minimal user input. A cluster is the general term for the computing backend of LiberTEM (even if it is confined to one machine), and is composed of one-or-more workers which each process some subset of dataset partitions as required.\n",
    "\n",
    "#### User Defined Functions (UDFs)\n",
    "\n",
    "UDFs are LiberTEM's mechanism to specify a function to run on a dataset. The format, while a bit more verbose than a simple Python function, enables the automatic scaling and partitioned processing described above. UDFs also allow the system to achieve maximum possible performance, particularly with respect to accessing data on the disk or network, which is often the slowest part of a computation when datasets are very large.\n",
    "\n",
    "#### Lazy computation\n",
    "\n",
    "LiberTEM relies heavily on *lazy computation* which is the concept that the system does the minimum necessary at any moment in order to reduce resource consumption. Reading data and computation is done at the last possible moment and resources are liberated as quickly as possible, meaning that no time or resource is wasted.\n",
    "\n",
    "More concretely, when you load a dataset with LiberTEM, nothing actually happens except building the *recipe* to **later** load the data when it is needed. The hard drive or network is barely solicitied. During computation, only the data which is currently being processed is loaded from disk; once computation on a chunk of data is complete it is removed from memory. As a result the RAM usage of the program rarely goes past a few gigabytes even when processing multi-hundred GB datasets.\n",
    "\n",
    "### This notebook:\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "- Creating a LiberTEM compute context / cluster\n",
    "- Loading a dataset\n",
    "- Running a simple function over a dataset\n",
    "- Running a full User Defined Function (UDF)\n",
    "- Running Brightfield, Annular Dark Field and CoM analyses\n",
    "- Diffraction spot matching\n",
    "- Creating and running a GPU-enabled custom UDF performing the Fourier Transform of each frame\n",
    "\n",
    "Firstly some setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232845e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def _plot_img(array, figsize=(16, 16), title=None, ax=None, fig=None, cbar=False, **kwargs):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "    im = ax.imshow(array, cmap=kwargs.pop('cmap', 'gray'), **kwargs)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    if cbar:\n",
    "        assert fig is not None\n",
    "        location = cbar if isinstance(cbar, str) else 'right'\n",
    "        fig.colorbar(im, ax=ax, location=location, fraction=0.05)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccd6d21",
   "metadata": {},
   "source": [
    "Download and extract a public dataset from Zenodo (https://zenodo.org/record/5113449), this may take a while:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410d4491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "dataset_path = pathlib.Path('./dataset/20200518 165148/default.hdr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d7177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "dataset_path.parent.parent.mkdir(exist_ok=True)\n",
    "if not dataset_path.is_file():\n",
    "    zip_path = dataset_path.parent.parent / '20200518 165148.zip'\n",
    "    URL = \"https://zenodo.org/record/5113449/files/20200518%20165148.zip\"\n",
    "    print('Downloading...')\n",
    "    urllib.request.urlretrieve(URL, filename=zip_path)\n",
    "    print('Done')\n",
    "    print('Extracting')\n",
    "    if dataset_path.parent.is_dir():\n",
    "        _ = [f.unlink() for f in dataset_path.parent.iterdir()]\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_path.parent.parent)\n",
    "    print('Done')\n",
    "    zip_path.unlink()\n",
    "else:\n",
    "    print('Datset already present')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f720ce",
   "metadata": {},
   "source": [
    "### Importing LiberTEM\n",
    "\n",
    "Importing libertem is similar to Hyperspy (and many other Python packages). The `lt` name is customary and is used to create most of the necessary objects. Here I also load `cluster_spec` and `DaskJobExecutor` because I want to demonstrate customising the size of the compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a8973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import libertem.api as lt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3dad37",
   "metadata": {},
   "source": [
    "### Specify the cluster size (optional but useful)\n",
    "\n",
    "We can specify the size of our compute resources with the `cluster_spec` function. The server 649 has 36 cores and 2 GPUs, which is too many for this demo (by default the cluster tries to use all resources).\n",
    "\n",
    "I specify 4 CPUs and 0 GPUs. Nothing happens yet, we just created a dictionary of workers that will be created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62866af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libertem.executor.dask import cluster_spec, DaskJobExecutor\n",
    "\n",
    "n_cpus = 4\n",
    "spec = cluster_spec(cpus=range(n_cpus), cudas=[], has_cupy=False)\n",
    "print([*spec.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f7e424",
   "metadata": {},
   "source": [
    "Now we create the `lt.Context` which is the entry point for most interaction with LiberTEM. To use the cluster specification we also need to tell LiberTEM which backend to use, here `DaskJobExecutor`. If we just called `lt.Context()` it would create a `DaskJobExecutor` cluster with all 36 CPUs and 2 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e1036",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = lt.Context(DaskJobExecutor.make_local(spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab76f941",
   "metadata": {},
   "source": [
    "The `lt.Context` object has a lot of methods on it, used to load data and run analyses, for example:\n",
    "\n",
    "- `ctx.load` : loads datasets\n",
    "- `ctx.run` : runs an `Analysis`\n",
    "- `ctx.run_udf` : runs a User Defined Function (UDF)\n",
    "- `ctx.map` : apply a function to every frame in a dataset\n",
    "- `ctx.create_disk_analysis` : create an `Analysis` object using a circular mask on each frame\n",
    "- ...\n",
    "\n",
    "### Load some data\n",
    "\n",
    "First we load some data.\n",
    "\n",
    "If the file(s) contain sufficient metadata, then the dataset shape will be automatically inferred, otherwise it might be necessary to set either or both of `nav_shape` and `sig_shape` when calling `load`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e4cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ctx.load('mib', path=str(dataset_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd30aa",
   "metadata": {},
   "source": [
    "By default LiberTEM supports a range of microscopy data formats, here I have loaded a custom format for the Precession Diffraction datasets used on the PFNC (this is custom code in env_TEM and not currently in the public version of LiberTEM).\n",
    "\n",
    "The formats supported are (normally):\n",
    "\n",
    "- Merlin Medipix (MIB)\n",
    "- Raw binary files\n",
    "- Digital Micrograph (DM3, DM4) files (image stacks, specifically, DM files are quite varied so not all flavours of DM files are suported)\n",
    "- EMPAD\n",
    "- K2IS\n",
    "- FRMS6\n",
    "- BLO\n",
    "- SER\n",
    "- HDF5\n",
    "- Norpix SEQ\n",
    "- MRC\n",
    "- TVIPS\n",
    "- Groups of raw files / CEA Precession data format\n",
    "- Dask arrays\n",
    "\n",
    "This gives us a dataset object `ds` which represents how the data should be read from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f01dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a7ac1f",
   "metadata": {},
   "source": [
    "At this stage no data has been read, the RAM usage for data is 0, we have only defined where the data is stored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9783baa",
   "metadata": {},
   "source": [
    "LiberTEM uses 'lazy computation' by default, which means nothing happens until it must happen, e.g. reading a file, and then only what needs to be read or computed is read or computed.\n",
    "\n",
    "### Nav and Sig dimensions\n",
    "\n",
    "LiberTEM uses the same concepts as Hyperspy for navigation and signal dimensions.\n",
    "\n",
    "- Each 'frame' is considered a signal\n",
    "- Each scan position is considered as a coordinate in the navigation dimensions\n",
    "\n",
    "**Attention** LiberTEM uses the same conventions as Python/Numpy for array indexing, i.e. `[y, x]` with `(0, 0)` top-left and positive `(down, right)`. Hyperspy reverses this and uses `[x, y]` !!\n",
    "\n",
    "The dimensions are both available in the `ds.meta` object (which contains many things):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe8edf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Navigation shape: {ds.meta.shape.nav}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db7897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Signal shape: {ds.meta.shape.sig}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bebdb02",
   "metadata": {},
   "source": [
    "One big difference between LiberTEM and HyperSpy datasets is that we cannot (easily) access the data like an array, because of lazy computation and the architecture of the system. This is not possible today, but might be in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d779ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ds[5, 10]\n",
    "except TypeError as e:\n",
    "    print(f'ERROR: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8f91e4",
   "metadata": {},
   "source": [
    "### Applying a function to a dataset\n",
    "\n",
    "So far, nothing has happened, but now we will carry out some (simple) computation, summing each frame to get a navigation-shaped result. To do this we need a function to apply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46947dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_frame(frame):\n",
    "    return np.sum(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786b5cf9",
   "metadata": {},
   "source": [
    "And we run it on the dataset using `ctx.map`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0029a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelsum = ctx.map(dataset=ds, f=sum_frame, progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeb8e59",
   "metadata": {},
   "source": [
    "With `ctx.map` the result is directly a numpy array which we can plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ff059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = _plot_img(pixelsum, title='Sum-over-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3395bc39",
   "metadata": {},
   "source": [
    "### Running a UDF\n",
    "\n",
    "A UDF is a User Defined Function, and is a way of specifying more complex analyses to run on a dataset.\n",
    "\n",
    "LiberTEM is distributed with a few in-built UDFs:\n",
    "\n",
    "- SumUDF \\[`libertem.udf.sum.SumUDF`\\]: Sum of frames every over every navigation coordinate (frame-shaped result)\n",
    "- LogsumUDF \\[`libertem.udf.logsum.LogsumUDF`\\]: Logarithmic sum over frames at every navigation coordinate (frame-shaped result)\n",
    "- SumSigUDF \\[`libertem.udf.sumsigudf.SumSigUDF`\\]: Get the sum of each frame in a dataset (navigation-shaped result)\n",
    "- StdDevUDF \\[`libertem.udf.stddev.StdDevUDF`\\]: Compute the statistics for each pixel over all frames (mean, std, var, sum)\n",
    "- PickUDF \\[`libertem.udf.raw.PickUDF`\\]: Load one or more frames from a dataset\n",
    "- FEMUDF \\[`libertem.udf.fem.FEMUDF`\\]: Fluctuation EM (standard deviation in ring around zero-order peak)\n",
    "- HoloReconstructUDF \\[`libertem.udf.holography.HoloReconstructUDF`\\]: Reconstruct complex electron wave using Fourier transform\n",
    "- CrystallinityUDF \\[`libertem.udf.crystallinity.CrystallinityUDF`\\]: Integrate a ring in the Fourier spectrum of each frame\n",
    "\n",
    "But these mostly exist as examples, and the user should implement what they want to do following the documentation.\n",
    "\n",
    "Let's run a simple built-in UDF and get the cumulative frame over all scan positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e41d189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libertem.udf.sum import SumUDF\n",
    "udf_results = ctx.run_udf(dataset=ds, udf=SumUDF(), progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c45c963",
   "metadata": {},
   "source": [
    "The `ctx.run_udf` method allows us to run one-or-more UDFs on the dataset (minimising the number of times the data is read from disk for each UDF). It can also take the following parameters:\n",
    "\n",
    "- `roi`: a boolean numpy array which limits the calculation to certain navigation coordinates\n",
    "- `corrections`: an object specifying things like dark field, gain map etc to pre-process the data before computation\n",
    "\n",
    "Results from UDFs are not directly a numpy array, as there can be more than one result per UDF, instead a dictionary is returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d727bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([*udf_results.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff01c46",
   "metadata": {},
   "source": [
    "Here we have only one result, the intensity of each pixel in the summed frame, the numpy array can be extracted by taking the `.data` attribute of the `'intensity'` result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db33625",
   "metadata": {},
   "outputs": [],
   "source": [
    "nav_sum_array = udf_results['intensity'].data\n",
    "print(f'nav_sum_array {type(nav_sum_array)} {nav_sum_array.shape} {nav_sum_array.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93980cf",
   "metadata": {},
   "source": [
    "This syntax is quite verbose but allows LiberTEM to handle many results and very sparse ROIs without making many full-sized copies of the results data.\n",
    "\n",
    "Now we can plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d572db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = _plot_img(nav_sum_array, figsize=(12, 12), title='Sum-over-nav', cbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c68b29",
   "metadata": {},
   "source": [
    "For construction of a UDF from scratch see the final example in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2148a3",
   "metadata": {},
   "source": [
    "### Masked analysis\n",
    "\n",
    "One of the inbuilt features of LiberTEM is very efficient summation of pixels which lie within a masked region (points, disks, rings). This is motivated by fast virtual detector, BF and ADF imaging.\n",
    "\n",
    "For the above dataset, we can compute a BF image using the following parameters (all in pixels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126acccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cx = 123. # centre of zero-order disk (x)\n",
    "cy = 127. # centre of zero-order disk (y)\n",
    "radius = 35. # brightfield radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1c52ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = _plot_img(nav_sum_array, figsize=(12, 12), title='BF Mask')\n",
    "disk = plt.Circle((cx, cy), radius, color='r', fill=False, linewidth=2)\n",
    "ax.add_patch(disk);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe06e8",
   "metadata": {},
   "source": [
    "With these parameters we create an analysis using the `ctx` and then run it on the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3a7246",
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_analysis = ctx.create_disk_analysis(dataset=ds, cx=cx, cy=cy, r=radius)\n",
    "bf_results = ctx.run(bf_analysis, progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4bcbc9",
   "metadata": {},
   "source": [
    "The results format is a little different, again we have an `intensity` result but we must access the numpy array using the `raw_data` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d61210",
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_image = bf_results['intensity'].raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e92c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = _plot_img(bf_image, title='Brightfield')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcce92b",
   "metadata": {},
   "source": [
    "And for completeness here is a virtual ADF image using `ctx.create_ring_analysis` but otherwise the same code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea08711",
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_analysis = ctx.create_ring_analysis(dataset=ds, cx=cx, cy=cy, ri=radius, ro=radius * 2.)\n",
    "adf_results = ctx.run(adf_analysis, progress=True)\n",
    "ax = _plot_img(adf_results['intensity'].raw_data, title='ADF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9bbe37",
   "metadata": {},
   "source": [
    "### CoM analysis\n",
    "\n",
    "LiberTEM also has a very fast centre-of-mass implementation, supporting whole-frame, disk and ring CoM with the option to correct for detector / scan rotation in the diffraction patterns.\n",
    "\n",
    "The same syntax is used, this time through `ctx.create_com_analysis`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71523eac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "com_analysis = ctx.create_com_analysis(dataset=ds, cx=cx, cy=cy, mask_radius=radius, scan_rotation=-20.)\n",
    "com_results = ctx.run(com_analysis, progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733fce19",
   "metadata": {},
   "source": [
    "For a CoM result set we have a number of outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6db45f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in com_results.keys():\n",
    "    print(f'{key}: {com_results[key].desc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf304697",
   "metadata": {},
   "source": [
    "Allowing us to compute things like electric field and charge density.\n",
    "\n",
    "And we can plot the raw data (though this datast is not great for CoM !):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b6ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
    "axs[0].imshow(com_results.x.raw_data, cmap='bwr');\n",
    "axs[0].set_title('X-component');\n",
    "axs[1].imshow(com_results.y.raw_data, cmap='bwr');\n",
    "axs[1].set_title('Y-component');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a4cf42",
   "metadata": {},
   "source": [
    "### Diffraction pattern interpretation (LiberTEM-Blobfinder)\n",
    "\n",
    "An optional add-on for LiberTEM is LiberTEM-Blobfinder, which is specialised in finding diffraction peaks and labelling them via a form of template matching. It can be installed with:\n",
    "\n",
    "```\n",
    "pip install libertem-blobfinder\n",
    "```\n",
    "\n",
    "On this particular dataset it is not interesting as there is only one peak, but for normal CBED patters this allows us to calculate the lattice vectors and subsequently the strain, or other tasks requiring finding diffraction spots.\n",
    "\n",
    "For this we need some additional parameters and imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b73a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import libertem.utils as ut\n",
    "import libertem.analysis.gridmatching as grm\n",
    "import libertem_blobfinder.udf.refinement as rf\n",
    "from libertem_blobfinder.common import patterns, correlation\n",
    "\n",
    "true_disk_r = 28.\n",
    "n_disks = 1\n",
    "search_radius_multiplier = 1.1\n",
    "outer_multiplier = 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bec832",
   "metadata": {},
   "source": [
    "First we must create our search template, LiberTEM has a few options built in, one of which is a 'background subtraction' mask which works well for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932cd29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_pattern = patterns.BackgroundSubtraction(radius=true_disk_r,\n",
    "                                               search=true_disk_r * search_radius_multiplier,\n",
    "                                               radius_outer=true_disk_r * outer_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13a2754",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = _plot_img(match_pattern.get_mask(ds.meta.shape.sig), title='Search pattern', figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32a41bc",
   "metadata": {},
   "source": [
    "We now apply this template to one frame to estimate the lattice for the dataset. We use the sum of all frames results computed earlier to increase the SNR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faac79b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks_yx = correlation.get_peaks(sum_result=nav_sum_array,\n",
    "                                 match_pattern=match_pattern,\n",
    "                                 num_peaks=n_disks)\n",
    "matcher = grm.Matcher(min_match=1)\n",
    "match = matcher.affinematch(peaks_yx, indices=np.asarray([[0, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae60369d",
   "metadata": {},
   "source": [
    "The `peaks_yx` object is a numpy array containing the positions `(y, x)` which had a good correlation score with the patterned defined above.\n",
    "\n",
    "The match object contains information about the matched lattice, specifically the estimated lattice coordinates and a selector object which identifies which peaks in `peaks_yx` are probably part of this lattice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4687a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = match.indices\n",
    "selector = match.selector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd40ddcc",
   "metadata": {},
   "source": [
    "The match object also contains also contains the estimated `zero` order position and `a` and `b` vectors as `(y, x)` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a48922",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_yx = match.zero\n",
    "a_vec = match.a\n",
    "b_vec = match.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e423b26",
   "metadata": {},
   "source": [
    "We can plot these results to see if they correspond to a reasonable lattice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0335ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = _plot_img(nav_sum_array, figsize=(12, 12), title='Lattice')\n",
    "\n",
    "peaks_yx_plot = peaks_yx[selector, :]\n",
    "labels = [f'({x}, {y})' for x, y in coords.reshape(-1, 2)]\n",
    "for (y, x), label in zip(peaks_yx_plot, labels):\n",
    "    disk = plt.Circle((x, y), true_disk_r, color='r', fill=False, linewidth=2)\n",
    "    ax.add_patch(disk);\n",
    "    ax.text(x, y, label, color='r', ha='center', va='center', fontsize='large', fontweight='bold')\n",
    "\n",
    "a_yx = np.stack([zero_yx, zero_yx + match.a], axis=0)\n",
    "b_yx = np.stack([zero_yx, zero_yx + match.b], axis=0)\n",
    "ax.plot(a_yx[:, 1], a_yx[:, 0], color='orange');\n",
    "ax.plot(b_yx[:, 1], b_yx[:, 0], color='lightgreen');\n",
    "ax.set_ylim(ds.meta.shape.sig[0], 0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a58e0fe",
   "metadata": {},
   "source": [
    "Now that we have a base lattice, we can fit it to every diffraction pattern in the dataset using the `run_refine` function.\n",
    "\n",
    "This can take a little while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ff85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_result, indices_refined = rf.run_refine(ctx=ctx,\n",
    "                                               dataset=ds,\n",
    "                                               zero=match.zero,\n",
    "                                               a=match.a,\n",
    "                                               b=match.b,\n",
    "                                               match_pattern=match_pattern,\n",
    "                                               match='affine',\n",
    "                                               indices=np.mgrid[0:1, 0:1],\n",
    "                                               matcher=matcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d020ca9e",
   "metadata": {},
   "source": [
    "The `indices` argument defines how many steps of the lattice vector the system should search. Here we have one disk so we only provide the (0, 0) index.\n",
    "\n",
    "`refine_result` has many separate results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc8459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, buf in refine_result.items():\n",
    "    print(key, buf.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3ce712",
   "metadata": {},
   "source": [
    "The shapes are from the navigation dimension which is `(20, 150)`, with `(7,)` peaks defined and contain an `(y, x)` last dimension of size `(2,)`. \n",
    "\n",
    "Within this object we have the refined lattice vectors `a` and `b` for each frame, the zero-order peak position `zero`, as well as evaluations of the fit quality and the peak positions.\n",
    "\n",
    "- `centers` are the `int` peak positions corresponding to the pixel with maximum correlation score\n",
    "- `refineds` are `float` coordinates which are the subpixel-refined positions\n",
    "\n",
    "`selector` is a boolean array indicating which peaks were identified in which diffraction patterns.\n",
    "\n",
    "Of course, in this dataset with only one diffraction spot the fitted lattice vectors are nonsensical, but otherwise they would have been estimated from the ensemble of detected spots in each frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddcdb3a",
   "metadata": {},
   "source": [
    "To show something I plot the relative changes to the positions of the central spot X/Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e97042",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pos = refine_result['refineds'].data[..., 0, 1]\n",
    "x_pos -= refine_result['refineds'].data[..., 0, 1].mean()\n",
    "ax = _plot_img(x_pos, cmap='bwr', title='Spot X-coord', cbar=True, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f6f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pos = refine_result['refineds'].data[..., 0, 0]\n",
    "y_pos -= refine_result['refineds'].data[..., 0, 0].mean()\n",
    "ax = _plot_img(refine_result['refineds'].data[..., 0, 0], cmap='bwr', title='Spot Y-coord', cbar=True, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6e4fcb",
   "metadata": {},
   "source": [
    "### Custom GPU-enabled / CuPy UDF [ADVANCED]\n",
    "\n",
    "As a (quite complex) example, I'll now demonstrate defining a custom UDF which is also CuPy-enabled (i.e. will use GPU when these are available in the LiberTEM backend), but will function identically (but slower) without a GPU.\n",
    "\n",
    "This is possible because the API of the library CuPy tries to be as close as possible to that of Numpy, and so in most cases the same code works with both backends. LiberTEM enables this by providing a special variable inside of a UDF: `self.xp` which acts as reference to either CuPy or Numpy as appropriate and available.\n",
    "\n",
    "The task we will perform with the GPU is trivial (and meaningless), we'll perform a Fourier transform on every frame and return two results:\n",
    "\n",
    "- the central pixel magnitude for each frame (a navigation-shaped result)\n",
    "- the sum Fourier transform over all frames (a signal-shaped result)\n",
    "\n",
    "To make a UDF we must first create a new UDF sub-class. For demonstration purposes I will define the UDF in separate chunks but in real code we would write the complete class definition in one cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f65c8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libertem.udf.base import UDF\n",
    "\n",
    "class FourierMean(UDF):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee77f4e",
   "metadata": {},
   "source": [
    "We must define some methods before this class will do what we want.\n",
    "\n",
    "Firstly we must tell LiberTEM that this is a `CuPy`-enabled UDF by defining the compatible backends:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8109c47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backends(self):\n",
    "    return ('numpy', 'cupy',)\n",
    "\n",
    "FourierMean.get_backends = get_backends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc0a83b",
   "metadata": {},
   "source": [
    "The tuple `('numpy', 'cupy',)` tells LiberTEM that this UDF will run on both numpy and cupy.\n",
    "\n",
    "Next we must define the results we want to get from running the UDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d54d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_buffers(self):\n",
    "    return {'central_magnitude': self.buffer(kind='nav', dtype='complex128', where='device'),\n",
    "            'sum_fourier': self.buffer(kind='sig', dtype='complex128', where='device')}\n",
    "\n",
    "FourierMean.get_result_buffers = get_result_buffers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fb6d71",
   "metadata": {},
   "source": [
    "A *Buffer* is an array into which we will put our results when running the UDF. The term buffer is used because often we will only be filling / viewing a small part of the whole array, and this corresponds to the behaviour of a block of memory (a buffer) which we allocate before computation and then fill bit-by-bit (and possibly out of order).\n",
    "\n",
    "The `self.buffer` method allows us to specify the results we will return, and the shape they will have, generally either 'nav' (for navigation / scan sized), or 'sig' (signal or frame size). We also declare the return data type and (in this case) that the result can be computed via a GPU ('device'). We must return these buffers in a dictionary where the key is a string which names the result.\n",
    "\n",
    "Now we must define what we want to do with each frame, this is the actual code which will be executed to compute our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae2b690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(self, frame):\n",
    "    # Compute the FFT using self.xp\n",
    "    frame_fft = self.xp.fft.fft2(frame)\n",
    "    # Put the results we derive from the FFT into the buffers we declared earlier\n",
    "    # The [:] ensures that we assign the results into the existing buffer rather than creating a new one\n",
    "    self.results.central_magnitude[:] = frame_fft[0, 0]\n",
    "    self.results.sum_fourier[:] += frame_fft        \n",
    "    \n",
    "FourierMean.process_frame = process_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaad2fa1",
   "metadata": {},
   "source": [
    "Here, rather than using `np.fft` directly to perform the fft, we instead use the special property `self.xp` provided by LiberTEM. This property will be either the library `CuPy` when the UDF is being run on a GPU, and will be `numpy` when run on a CPU, allowing the code to work in both contexts. This is possible as CuPy's API is very close to that of Numpy.\n",
    "\n",
    "We must also tell LiberTEM how to combine results from different parts of the dataset, we do this via the `merge` method. Again note the `[:]` which ensures we are assigning into the buffer rather than re-creating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bda1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(self, dest, src):\n",
    "    dest.central_magnitude[:] = src.central_magnitude\n",
    "    dest.sum_fourier[:] += src.sum_fourier\n",
    "    \n",
    "FourierMean.merge = merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396b89b6",
   "metadata": {},
   "source": [
    "`dest` and `src` refer to the results for a the whole dataset (destination buffer) and for a single partition (source buffer), respectively.\n",
    "\n",
    "Finally we can run the UDF. \n",
    "\n",
    "NOTE: Our current `lt.Context` only has a few CPU workers so this will act as a baseline for non-GPU execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0610a897",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ctx.run_udf(dataset=ds, udf=FourierMean(), progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803bbfee",
   "metadata": {},
   "source": [
    "To get a comparison, we next create a second context / backend with some GPU workers. Dask/Distributed complains because we already have a cluster running, but this is not a problem.\n",
    "\n",
    "If running on a machine without CuPy (without a GPU) then the rest of this notebook will fail as we are explicitly creating a CuPy-enabled cluster with no other workers! (normally you would want a mixed cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2a0e10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spec_gpu = cluster_spec(cpus=[], cudas=[0, 1], has_cupy=True)\n",
    "ctx_gpu = lt.Context(DaskJobExecutor.make_local(spec_gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aecc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    res = ctx_gpu.run_udf(dataset=ds, udf=FourierMean(), progress=True)\n",
    "except ModuleNotFoundError:\n",
    "    print('No CuPy available')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908ed53c",
   "metadata": {},
   "source": [
    "We should see that the GPU-based cluster is much faster for this UDF, even though the code that is running is exactly the same. It will perform optimally on a mixed GPU and CPU cluster with no modifications.\n",
    "\n",
    "The results are not very interesting, but included for completeness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa2f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = _plot_img(np.fft.fftshift(np.log(np.abs(res['sum_fourier'].data))),\n",
    "               title='Sum Fourier (log color)',\n",
    "               figsize=(12, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96117859",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = _plot_img(np.abs(res['central_magnitude'].data), title='Central Fourier magnitude')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
